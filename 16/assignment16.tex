\documentclass[twocolumn]{article}
\input{../common.tex}

\newcommand{\phat}{\ensuremath{\hat{p}}}
\newcommand{\lhat}{\hat{\lambda}}
%\newcommand{\max}{\mathrm{max}}
\numberwithin{equation}{section}
\renewcommand{\theequation}{\arabic{equation}}
\newcommand{\sdiff}{\Sigma d_i^2}

\title{MATH 308 Assignment 16:\\Exercises 6.4}
\date{April 10, 2014}

\begin{document}
\maketitle

\setsection{1}
We know that $X\sim\mathcal{B}(n,p)\implies f_X(x)=\binom{n}{x}p^x(1-p)^{n-x}$. Thus, the probability of obtaining the value $X$ can be written as a function of the parameter $p$:
\begin{gather*}
L(p)=\binom{n}{X}p^X(1-p)^{n-X}\\
\begin{aligned}
\implies L'/\binom{n}{X}=&Xp^{X-1}(1-p)^{n-X}\\
&-p^X(n-X)(1-p)^{n-X-1}
\end{aligned}
\end{gather*}
We can then obtain the MLE \phat{} by setting $L'=0$:\begin{gather*}
Xp^{X-1}(1-p)^{n-X}=p^X(n-X)(1-p)^{n-X-1}\\
\implies \phat(n-X)=X(1-\phat)\\
\implies \phat n-\cancel{\phat X}=X-\cancel{X\phat}\\
\implies \phat=X/n \qed
\end{gather*}

\setsection{2}
$X\sim\mathcal{P}(\lambda)\implies f_X=\frac{\lambda^xe^{-\lambda}}{x!}$. So, the likelihood of obtaining the sample $\{x_1,x_2,\ldots x_n\}$ is $L(\lambda)=\prod_{i=1}^nf_X(x_i)$, since the elements of the sample are i.i.d.

\begin{gather*}
\begin{aligned}
L&=\prod_{i=1}^n\frac{\lambda^{x_i}e^{-\lambda}}{x_i!}\\
&=\frac{\lambda^{\Sigma x}e^{-n\lambda}}{\prod (x_i!)}=\lambda^{\Sigma x}e^{-n\lambda}/\mathcal{C}\\
\end{aligned}\displaybreak\\
\implies L\mathcal{C}=\lambda^{\Sigma x}e^{-n\lambda}\\
\implies \log L+\log\mathcal{C}=\Sigma x\log\lambda-n\lambda\\
\implies L'/L=\Sigma x/\lambda-n\\
\implies 0=\Sigma x/\hat\lambda-n\\
\implies\lhat= \Sigma x_i/n = \overline{x}\qed
\end{gather*}

\setsection{4}
As in the previous problem,\begin{gather*}
\begin{aligned}
L(\theta)&=\prod_{i=1}^n\frac{x_i^3e^{-x_i/\theta}}{6\theta^4}\\
&=\frac{(\Pi x)^3e^{-\Sigma x/\theta}}{6^n\theta^{4n}}\\
&=\mathcal{C}e^{-\Sigma x/\theta}\theta^{-4n}\\
\end{aligned}\\
\implies \log L-\log C=-\Sigma x/\theta-4n\log\theta\\
\implies L'/L=\Sigma x/\theta^2-4n/\theta\\
\implies 0=\frac{\Sigma x}{\hat{\theta}}-4n\\
\implies \hat{\theta}=\frac{\Sigma x}{4n}=\overline{x}/4
\end{gather*}

\setsection{5}
\subsection{}\begin{gather*}
\begin{aligned}
L(\mu)&=\prod_{i=1}^n\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\\
&=\mathcal{C}\prod_{i=1}^ne^{-\frac{x_i^2+\mu^2-2x_i\mu}{2\sigma^2}}\\
&=\mathcal{C}\prod_{i=1}^n(e^{\frac{x_i\mu}{\sigma^2}}/e^{\frac{\mu^2}{2\sigma^2}})\\
&=\mathcal{C}e^{\frac{\mu\Sigma x}{\sigma^2}}/e^{\frac{n\mu^2}{2\sigma^2}}\\
&=\mathcal{C}e^\frac{2\mu\Sigma x-n\mu^2}{2\sigma^2}
\end{aligned}\\
\implies \log L-\log\mathcal{C}=\frac{2\mu\Sigma x-n\mu^2}{2\sigma^2}\\
\implies L'/L=\frac{2\Sigma x- 2n\mu}{2\sigma^2}\\
\implies 0=\Sigma x-n\hat{\mu}\\
\implies \hat{\mu}=\Sigma x/n=\overline{x}
\end{gather*}

\subsection{}
\begin{gather*}
\begin{aligned}
L(\sigma)&=\prod_{i=1}^n\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\\
&=\frac{\mathcal{C}}{\sigma^n}\prod_{i=1}^ne^{\frac{\sdiff}{2\sigma^2}}\\
\end{aligned}\\
\implies L/\mathcal{C}=e^\frac{\sdiff}{2\sigma^2}/\sigma^n\\
\implies \log L-\log\mathcal{C}=-\frac{\sdiff}{2\sigma^2}-n\log\sigma\\
\implies L'/L=\sdiff/\sigma^3-n/\sigma\\
\implies 0=\sdiff/\hat\sigma^2-n\displaybreak\\
\begin{aligned}
\implies \hat\sigma^2&=\sdiff/n\\
&=\Sigma(X_i-\mu)^2/n
\end{aligned}\\
\implies\hat\sigma=\sqrt{\Sigma(X_i-\mu)^2/n}
\end{gather*}


\setsection{10}
\begin{gather*}
\begin{aligned}
	L(\mu)&=\frac{1}{\sigma_X\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma_X^2}}
	\frac{1}{\sigma_Y\sqrt{2\pi}}e^{-\frac{(y-1.3\mu)^2}{2\sigma_Y^2}}\\
	&=\mathcal{C}e^{-\frac{(x-\mu)^2}{2\sigma_X^2}-\frac{(y-1.3\mu)^2}{2\sigma_Y^2}}
\end{aligned}\\
\implies\log L-\log\mathcal{C}=-\frac{(x-\mu)^2}{2\sigma_X^2}-\frac{(y-1.3\mu)^2}{2\sigma_Y^2}\\
\implies L'/L=\frac{(x-\mu)}{\sigma_X^2}+\frac{(y-1.3\mu)}{\sigma_Y^2}\\
\implies 0=\frac{95-\hat\mu}{15^2}+\frac{130-1.3\hat\mu}{20^2}\\
\implies \hat\mu\approx97.1
\end{gather*}


\setsection{12}

\begin{gather}
\begin{aligned}
L(r,\lambda)&=\prod_{i=1}^n\frac{\lambda^r}{\Gamma(r)}x_i^{r-1}e^{-\lambda x_i}\nonumber\\
&=\frac{\lambda^{nr}}{\Gamma^n(r)}(\Pi x)^{r-1}e^{-\lambda\Sigma x}\nonumber\\
\end{aligned}\\
\log L=nr\log\lambda+(r-1)\log(\Pi x)\nonumber\\
-\lambda\Sigma x-n\log\Gamma(r)\nonumber\\
\implies \frac{1}{L}\frac{\partial L}{\partial r}=n\log\lambda+\log(\Pi x)-n\psi(r)\nonumber\\
\implies n\psi(\hat r)=n\log\hat\lambda+\log(\Pi x)\\
\text{And }\frac{1}{L}\frac{\partial L}{\partial \lambda}=nr/\lambda-\Sigma x\nonumber\\
\implies \hat\lambda=\frac{n\hat r}{\Sigma x}
\end{gather}

\newpage

\setsection{14}
\begin{gather}
X=\{2,3,5,9,10\}\nonumber\\
f(x)=\frac{1}{\beta-\alpha+1},\alpha\le x\le\beta\nonumber\\
\implies \ev{X^k}=\sum_{x=\alpha}^\beta {\frac{x^k}{\beta-\alpha+1}}\nonumber\\
\begin{aligned}
\implies \overline X
&=\frac{\sum_{x=\hat\alpha}^{\hat\beta} x}{\hat\beta-\hat\alpha+1}\\
&=(\hat\beta+\hat\alpha)/2\label{eq1}\\
\end{aligned}\\
\begin{aligned}
\text{And }\overline{X^2}
&=\frac{\sum_{x=\hat\alpha}^{\hat\beta} x^2}{\hat\beta-\hat\alpha+1}\\
&=(\hat\beta-\hat\alpha+2\hat\alpha^2\hat\beta^2+2\hat\beta^2+2\hat\alpha^2)/6\label{eq2}
\end{aligned}
\end{gather}
Where $\overline X=\Sigma X/n=5.8$ and $\overline{X^2}=\Sigma X^2/n=43.8$. Solving \ref{eq1} and \ref{eq2}, we get:\begin{gather*}
\hat\alpha=\overline X-\frac{\sqrt{12\overline{X}^2+12\overline{X^2}}-1}{2}\approx 1\\
\hat\beta=\overline X+\frac{\sqrt{12\overline{X}^2+12\overline{X^2}}-1}{2}\approx 11
\end{gather*}


\setsection{16}
\subsection{}
\begin{gather}
\ev{X}=r/\lambda\nonumber\\
\implies \overline X=\hat r/\hat \lambda\label{eq161}\\
\begin{aligned}
\ev{X^2}&=\text{Var}(X)+\ev{X}^2\\
&=r/\lambda^2+r^2/\lambda^2\\
&=r(1+r)/\lambda\\
\end{aligned}\nonumber\\
\implies \overline{X^2}=\hat r(1+\hat r)/{\hat{\lambda}}^2\label{eq162}
\end{gather}
Solving \ref{eq161} and \ref{eq162},
\[
r=\frac{\overline{X}^2}{\overline{X^2}-\overline{X}^2}\approx 2.67
\]\[
\lambda=\frac{\overline{X}}{\overline{X^2}-\overline{X}^2}\approx 3.84
\]

\subsection{}
We run a test on the null hypothesis $H_0$ that the data are $\sim\Gamma(\hat r,\hat\lambda)$, against the alternative hypothesis $H_A$ that they are not. Running the goodness-of-fit test yields a $p$-value of 0.78, allowing us to strongly favour $H_0$.

\subsection{}
\img{16a.pdf}
\img{16b.pdf}

\setsection{25}
\begin{align*}
\ev{X}&=\ev{\sum_{i=1}^n a_iX_i}\\
&=\sum_{i=1}^n a_i\ev{X_i}\\
&=\sum_{i=1}^n a_i\mu\\
&=\mu\Sigma a
\end{align*}
For an unbiased estimator of $\mu$,\begin{gather*}
\ev{X}=\mu\\
\implies \cancel{\mu}\Sigma a=\cancel{\mu}\\
\implies \Sigma a=1
\end{gather*}

\setsection{27}
\subsection{}
\begin{align*}
\hat\sigma^2&=\frac{1}{n}\Sigma(x_i-\bar x)^2\\
&=\frac{\sigma^2}{\sigma^2}\frac{n-1}{n}\frac{1}{n-1}\Sigma(x_i-\bar x)^2\\
&=\frac{\sigma^2}{n}\frac{(n-1)S^2}{\sigma^2}
\end{align*}
Where $\frac{(n-1)S^2}{\sigma^2}\sim\chi_{n-1}^2$.

\begin{gather*}
\begin{aligned}
\ev{\hat{\sigma}^2}
&=\ev{\frac{\sigma^2}{n}\frac{(n-1)S^2}{\sigma^2}}\\
&=\frac{\sigma^2}{n}\ev{\frac{(n-1)S^2}{\sigma^2}}\\
&=\frac{\sigma^2}{n}(n-1)=\sigma^2(1-1/n)
\end{aligned}\\
\begin{aligned}
\implies \mathrm{Bias}(\hat\sigma^2)&=\ev{\hat\sigma^2}-\sigma^2\\
&=\sigma^2(1-1/n-1)=-\sigma^2/n
\end{aligned}
\end{gather*}

\subsection{}
\begin{align*}
\var\hat\sigma^2
&=\frac{\sigma^4}{n^2}\var\frac{(n-1)S^2}{\sigma^2}\\
&=\frac{\sigma^4}{n^2}2(n-1)=2\sigma^4(n-1)/n^2
\end{align*}

\subsection{}\begin{align*}
\mathrm{MSE}
&=\mathrm{Var}(\hat\sigma^2)+\mathrm{Bias}^2(\hat\sigma^2)\\
&=\frac{2\sigma^4(n-1)}{n^2}+\frac{\sigma^4}{n^2}\\
&=\sigma^4(2n-1)/n^2
\end{align*}
\setsection{34}

\subsection{}
\begin{gather*}
f(x)=6x^5/\theta^6\\
\implies F(x)=\intg{0}{x}{6x^5/\theta^6}{t}=x^6/\theta^6\\
\begin{aligned}
f_{X,\max}
&= f_{(n)}(x)\\
&=nf(x)F^{n-1}(x)\\
&=n\frac{6x^5}{\theta^6}\frac{x^{6n-6}}{\theta^{6n-6}}\\
&=6nx^{6n-1}/\theta^{6n}
\end{aligned}
\end{gather*}

\subsection{}
\begin{gather*}
\begin{aligned}
\ev{X_{\max}}
&=\intg{0}{\theta}{x\frac{6nx^{6n-1}}{\theta^{6n}}}{x}\\
&=\frac{6n}{\theta^{6n}}\left.\frac{x^{6n+1}}{6n+1}\right|_0^\theta\\
&=\frac{6n}{6n+1}\theta
\end{aligned}
\end{gather*}

\subsection{}
\begin{align*}
\bias
&=\ev{X_{\max}}-\theta\\
&=\theta\left(\frac{6n}{6n+1}-1\right)\\
&=\frac{-\theta}{6n+1}
\end{align*}

\subsection{}
\begin{gather*}
\begin{aligned}
\ev{X_{\max}^2}
&=\intg{0}{\theta}{x^2\frac{6nx^{6n-1}}{\theta^{6n}}}{x}\\
&=6n\theta^2/(6n+2)\\
\end{aligned}\\
\begin{aligned}
\var&=EX_{\max}^2-E^2X_{\max}\\
&=\frac{3 \theta ^2 n}{(3 n+1) (6 n+1)^2}\\
\end{aligned}\\
\mse=\var+\bias^2=\frac{\theta ^2}{18 n^2+9 n+1}
\end{gather*}

\setsection{36}
\subsection{}\begin{align*}
\ev{W}
&=\ev{a\overline X}+\ev{(1-a)\overline Y}\\
&=a\e\overline X+(1-a)\e\overline Y\\
&=a\mu+(1-a)\mu=1
\end{align*}

\subsection{}\begin{gather*}
\begin{aligned}
\var W
&=\var{(a\bar X+(1-a)\bar Y)}\\
&=a^2 \var\bar X+(1-a)^2\var\bar Y\\
&=a^2\sigma_1^2/n+(1-a)^2\sigma_2^2/m\\
\end{aligned}\\
\implies \var{'(W)}=2a\sigma_1^2/n-2(1-a)\sigma_2^2/m\\
\implies \hat a\sigma_1^2/n=\sigma_2^2/m-\hat a\sigma_2^2/m\\
\implies \hat a=\frac{\sigma_2^2/m}{\sigma_1^2/n+\sigma_2^2/m}
\end{gather*}

\end{document}